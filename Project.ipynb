{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b14cc62e",
   "metadata": {},
   "source": [
    "# Understanding Hired Rides in NYC\n",
    "\n",
    "_[Project prompt](https://docs.google.com/document/d/1VERPjEZcC1XSs4-02aM-DbkNr_yaJVbFjLJxaYQswqA/edit#)_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b9c773",
   "metadata": {},
   "source": [
    "### By Zhonghao Wang( zw2865 ) and Yanyi Wang (yw3949)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4733c3c",
   "metadata": {},
   "source": [
    " This part we are setting up the notebook by importing necessary libraries and installing packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86eec839",
   "metadata": {},
   "source": [
    "## Project Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "257639d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65fd033b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all import statements needed for the project:\n",
    "\n",
    "import math\n",
    "import bs4\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import requests\n",
    "import sqlalchemy as db\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import stat\n",
    "import scipy.stats as st\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a51b1e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Global Variables needed for the projects:\n",
    "\n",
    "TAXI_URL = \"https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "TAXI_ZONES = \"taxi_zones/taxi_zones.shp\"\n",
    "# add other constants to refer to any local data, e.g. uber & weather\n",
    "UBER_CSV = \"uber_rides_sample.csv\"\n",
    "WEATHER_PATH = \"weather\"\n",
    "NEW_YORK_BOX_COORDS = ((40.560445, -74.242330), (40.908524, -73.717047))\n",
    "DATABASE_URL = \"sqlite:///project.db\"\n",
    "DATABASE_SCHEMA_FILE = \"schema.sql\"\n",
    "QUERY_DIRECTORY = \"queries\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd2b0532",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load shp file and get coordinations of pickup and dropoff location\n",
    "taxi_zones_df = gpd.read_file(TAXI_ZONES)\n",
    "taxi_zones_df = taxi_zones_df.to_crs(4326)\n",
    "taxi_zones_df['lon'] = taxi_zones_df.centroid.x  \n",
    "taxi_zones_df['lat'] = taxi_zones_df.centroid.y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba387c06",
   "metadata": {},
   "source": [
    "## Part 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebe078a",
   "metadata": {},
   "source": [
    "In part 1 we \n",
    "1. Calculates the distance between two coordinates in kilometers\n",
    "2. Download all yellow taxi trip data from nyc government webiste using web scraping.\n",
    "3. Load and clean the yellow taxi data for each month from 2009/01 to 2015/06\n",
    "4. Load and clean the uber data.\n",
    "5. Load and clean the weather data from 2009 to 2015."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcb50d2",
   "metadata": {},
   "source": [
    "### Calculating distance\n",
    "We defined a function that calculates the distance between two coordinates in kilometers that using latitude and lontitue corrdination. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3221491",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distance(from_coord, to_coord):\n",
    "    \"\"\"\n",
    "    This function calculated distance from one coordination to another\n",
    "    \n",
    "    Parameters: from_coord:input pickup location lon and lat coordinations dataframe\n",
    "                to_coord: input dropoff location lon and lat coordinations dataframe\n",
    "    return: distance in km\n",
    "    \n",
    "    \"\"\"\n",
    "    # convert longitude and latitude to radian\n",
    "    lon1, lat1, lon2, lat2 = map(math.radians, from_coord+to_coord) \n",
    "    dlon=lon2-lon1\n",
    "    dlat=lat2-lat1\n",
    "    a=math.sin(dlat/2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon/2)**2\n",
    "    distance=2*math.asin(math.sqrt(a))* 6371*1000\n",
    "    distance=round(distance/1000,3)\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07f6295",
   "metadata": {},
   "source": [
    "### Create a new column \n",
    "We defined a function that create a new column called distance in the dataframe and calculated distance for \n",
    "each row in the dataframe for columns that contain pickup latitude lontitude and drop off lat and lon.The result distance will be the distance in kilometers between pick up coordination and drop off coordination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c716cca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_distance_column(dataframe):\n",
    "    \"\"\"\n",
    "    This function add distance column to dataframe \n",
    "    \n",
    "    Parameters: dataframe contains pick up drop off lat and lon coordinates\n",
    "    return: new dataframe\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    dataframe['distance'] = dataframe.apply(lambda x: calculate_distance((\n",
    "        x['pickup_longitude'], x['pickup_latitude']), (x['dropoff_longitude'], x['dropoff_latitude'])), axis=1)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a764fc70",
   "metadata": {},
   "source": [
    "### Processing Taxi Data\n",
    "1. We use request to scrap the webiste of nyc govnment to get Yellow Taxi Trip Records data by first get all links of data for each month from 2009/01 to 2015/06 by find_taxi_parquet_urls(taxi_url) funciton \n",
    "2. After we get all the links for data, we used get_month_taxi_data(taxi_data_url) to download all the taxt data      files\n",
    "3. We clean the data for each month by removing unnecessary columns, renaming some columns and delete some trips      that are not of our interest by clean_month_taxi_data(raw_taxi_data,taxi_zones) function\n",
    "4. get_and_clean_taxi_data() is getting the cleaned taxt data we are going to use for later parts of the project,    we also did some sampling to match the number of data with Uber date we are going to use next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50516ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_taxi_parquet_urls(taxi_url):\n",
    "    \"\"\"\n",
    "    This function finds urls of the parquet files. \n",
    "    \n",
    "    Parameters: taxi_url : the website contains all links parquet files\n",
    "    \n",
    "    return: a list of parquet file urls \n",
    "    \n",
    "    \"\"\"\n",
    "    response = requests.get(taxi_url)\n",
    "    html = response.content\n",
    "    results_page = bs4.BeautifulSoup(html, 'html.parser')\n",
    "    links = results_page.find_all('a',title=r'Yellow Taxi Trip Records')\n",
    "    pattern = r'yellow_tripdata_(2009|201[0-4]|2015-0[1-6])'\n",
    "    urls = [link.get('href') for link in links if re.search(pattern,link.get('href'))]\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e52f267d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_month_taxi_data(taxi_data_url):\n",
    "    \"\"\"\n",
    "    This function download and get all taxi data from each month  \n",
    "    \n",
    "    Parameters: taxi_data_url : a link to the file contain taxi data of one month \n",
    "    \n",
    "    return: a dataframe contain this month data \n",
    "    \n",
    "    \"\"\"\n",
    "    pattern = r'yellow_tripdata_[0-9]{4}-[0-9]{2}'\n",
    "    name = re.search(pattern,taxi_data_url).group()\n",
    "    path = os.path.join(os.getcwd(),'taxi',name+'.csv')\n",
    "    if os.path.exists(path):\n",
    "        print('read',name)\n",
    "        df = pd.read_csv(path)\n",
    "    else:\n",
    "        print('download',name)\n",
    "        df = pd.read_parquet(taxi_data_url, engine='pyarrow')\n",
    "        df.to_csv(path)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ecadf3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_taxi_data(raw_taxi_data,taxi_zones):\n",
    "    \"\"\"\n",
    "    This function clean all taxi data from each month  \n",
    "    \n",
    "    Parameters: raw_taxi_data : raw data before cleaning \n",
    "                taxi_zones: dataframe we get that contains pickup,dropoff lat and lon\n",
    "    \n",
    "    return: a cleaned dataframe contain this month data \n",
    "    \n",
    "    \"\"\"\n",
    "    taxi_data = pd.DataFrame()\n",
    "    if 'tpep_pickup_datetime' in raw_taxi_data.columns:\n",
    "        taxi_data['pickup_datetime'] = raw_taxi_data['tpep_pickup_datetime']\n",
    "    elif 'Trip_Pickup_DateTime' in raw_taxi_data.columns:\n",
    "        taxi_data['pickup_datetime'] = raw_taxi_data['Trip_Pickup_DateTime']\n",
    "    elif 'pickup_datetime' in raw_taxi_data.columns:\n",
    "        taxi_data['pickup_datetime'] = raw_taxi_data['pickup_datetime']\n",
    "    else:\n",
    "        raise ValueError('No pickup_datetime')\n",
    "    if 'PULocationID' and 'DOLocationID' in raw_taxi_data.columns:\n",
    "        taxi_data['PULocationID'] = raw_taxi_data['PULocationID']\n",
    "        taxi_data['DOLocationID'] = raw_taxi_data['DOLocationID']\n",
    "        taxi_data = taxi_data.merge(taxi_zones[['LocationID','lon','lat']].set_index('LocationID'),\n",
    "                                    left_on='PULocationID', right_on='LocationID')\n",
    "        taxi_data = taxi_data.rename(columns={'lon': 'pickup_longitude', 'lat': 'pickup_latitude'})\n",
    "        taxi_data = taxi_data.merge(taxi_zones[['LocationID','lon','lat']].set_index('LocationID'),\n",
    "                                    left_on='DOLocationID', right_on='LocationID')\n",
    "        taxi_data = taxi_data.rename(columns={'lon': 'dropoff_longitude', 'lat': 'dropoff_latitude'})\n",
    "\n",
    "        taxi_data = taxi_data.drop(columns=['PULocationID','DOLocationID'])\n",
    "    elif 'Start_Lon' and 'Start_Lat' and 'End_Lon' and 'End_Lat' in raw_taxi_data.columns:\n",
    "        taxi_data['pickup_longitude'] = raw_taxi_data['Start_Lon']\n",
    "        taxi_data['pickup_latitude'] = raw_taxi_data['Start_Lat']\n",
    "        taxi_data['dropoff_longitude'] = raw_taxi_data['End_Lon']\n",
    "        taxi_data['dropoff_latitude'] = raw_taxi_data['End_Lat']\n",
    "    elif 'pickup_longitude' and 'pickup_latitude' and 'dropoff_longitude' and 'dropoff_latitude' in raw_taxi_data.columns:\n",
    "        taxi_data['pickup_longitude'] = raw_taxi_data['pickup_longitude']\n",
    "        taxi_data['pickup_latitude'] = raw_taxi_data['pickup_latitude']\n",
    "        taxi_data['dropoff_longitude'] = raw_taxi_data['dropoff_longitude']\n",
    "        taxi_data['dropoff_latitude'] = raw_taxi_data['dropoff_latitude']\n",
    "    else:\n",
    "        raise ValueError('No pickup_datetime')\n",
    "\n",
    "    taxi_data['pickup_datetime'] = pd.to_datetime(taxi_data['pickup_datetime'])\n",
    "    taxi_data = taxi_data.dropna()\n",
    "    \n",
    "    \n",
    "    #pick up in bounding box \n",
    "    taxi_data = taxi_data[(taxi_data['pickup_longitude']>NEW_YORK_BOX_COORDS[0][1])\n",
    "                          & (taxi_data['pickup_longitude']<NEW_YORK_BOX_COORDS[1][1])]\n",
    "    taxi_data = taxi_data[(taxi_data['pickup_latitude']>NEW_YORK_BOX_COORDS[0][0])\n",
    "                          & (taxi_data['pickup_latitude']<NEW_YORK_BOX_COORDS[1][0])]\n",
    "    #drop off in bounding box  \n",
    "    taxi_data = taxi_data[(taxi_data['dropoff_longitude']>NEW_YORK_BOX_COORDS[0][1])\n",
    "                          & (taxi_data['dropoff_longitude']<NEW_YORK_BOX_COORDS[1][1])]\n",
    "    taxi_data = taxi_data[(taxi_data['dropoff_latitude']>NEW_YORK_BOX_COORDS[0][0])\n",
    "                          & (taxi_data['dropoff_latitude']<NEW_YORK_BOX_COORDS[1][0])]\n",
    "    if 'Tip_Amt'  in raw_taxi_data.columns:\n",
    "        taxi_data['tip_amount'] = raw_taxi_data['Tip_Amt']\n",
    "       \n",
    "        \n",
    "    elif  'tip_amount' in raw_taxi_data.columns:\n",
    "         taxi_data['tip_amount'] = raw_taxi_data['tip_amount']\n",
    "    \n",
    "    return taxi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "377dd9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_taxi_data(taxi_zones):\n",
    "    \"\"\"\n",
    "    This function get and load and clean by calling functions we created above  \n",
    "    \n",
    "    Parameters: taxi_zones: dataframe we get that contains pickup,dropoff lat and lon\n",
    "    \n",
    "    return: A dataframe contain all taxi data we need \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    all_csv_urls = find_taxi_parquet_urls(TAXI_URL)\n",
    "    all_taxi_dataframes = []\n",
    "    for csv_url in all_csv_urls:\n",
    "        month_taxi_dataframe = get_month_taxi_data(csv_url)\n",
    "        month_taxi_dataframe = clean_month_taxi_data(month_taxi_dataframe,taxi_zones)\n",
    "        month_taxi_dataframe = add_distance_column(month_taxi_dataframe)\n",
    "        all_taxi_dataframes.append(month_taxi_dataframe.sample(3000,random_state=1))\n",
    "        \n",
    "    # create one gigantic dataframe with data from every month needed\n",
    "    all_taxi_data = pd.concat(all_taxi_dataframes)\n",
    "    all_taxi_data = all_taxi_data.sample(200000,random_state=1)\n",
    "    return all_taxi_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e21f38e",
   "metadata": {},
   "source": [
    "### Processing Uber Data\n",
    "1. We clean the uber data by removing unnecessary columns, renaming some columns and delete some trips that are not in bound of nyc by clean_uber_data(csv_file) function\n",
    "4. get_uber_data() is getting the cleaned taxt data we are going to use for later parts of the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "644a783a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_uber_data(csv_file):\n",
    "    \"\"\"\n",
    "    This function get and load and clean uber data   \n",
    "    \n",
    "    Parameters: csv_file: the file path of uber data \n",
    "    \n",
    "    return: uber data in a dataframe we need \n",
    "    \n",
    "    \"\"\"\n",
    "    uber_data = pd.read_csv(csv_file)\n",
    "    uber_data = uber_data.drop(columns=['Unnamed: 0','key','fare_amount','passenger_count'])\n",
    "    uber_data = uber_data.dropna()\n",
    "    \n",
    "    #pick up in bounding box \n",
    "    uber_data = uber_data[(uber_data['pickup_longitude']>NEW_YORK_BOX_COORDS[0][1]) & (uber_data['pickup_longitude']<NEW_YORK_BOX_COORDS[1][1])]\n",
    "    uber_data = uber_data[(uber_data['pickup_latitude']>NEW_YORK_BOX_COORDS[0][0]) & (uber_data['pickup_latitude']<NEW_YORK_BOX_COORDS[1][0])]\n",
    "    #drop off in bounding box  \n",
    "    uber_data = uber_data[(uber_data['dropoff_longitude']>NEW_YORK_BOX_COORDS[0][1]) & (uber_data['dropoff_longitude']<NEW_YORK_BOX_COORDS[1][1])]\n",
    "    uber_data = uber_data[(uber_data['dropoff_latitude']>NEW_YORK_BOX_COORDS[0][0]) & (uber_data['dropoff_latitude']<NEW_YORK_BOX_COORDS[1][0])]\n",
    "    uber_data['pickup_datetime'] = pd.to_datetime(uber_data['pickup_datetime'])\n",
    "    \n",
    "    return uber_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ec99879",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uber_data():\n",
    "    \"\"\"\n",
    "    This function get and load and clean uber data by calling functions above \n",
    "    and add distance between coordinates\n",
    "    \n",
    "    Parameters: \n",
    "    \n",
    "    return: all uber data in a dataframe we need \n",
    "    \n",
    "    \"\"\"\n",
    "    uber_dataframe = clean_uber_data(UBER_CSV)\n",
    "    uber_dataframe = add_distance_column(uber_dataframe)\n",
    "    return uber_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5f36f2",
   "metadata": {},
   "source": [
    "Get all the uber_data in a dataframe by calling function we defined above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9ee79d",
   "metadata": {},
   "source": [
    "### Processing Weather Data\n",
    "\n",
    "1. We clean the weather data by removing unnecessary columns, renaming some columns and get hourly, daily weather data using clean_month_weather_data_hourly(csv_file) and clean_month_weather_data_daily(csv_file) functions.\n",
    "4.  We use load_and_clean_weather_data()to load and put everything in a single dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "385919ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_hourly(csv_file):\n",
    "    \"\"\"\n",
    "    This function get and load and clean hourly weather data   \n",
    "    \n",
    "    Parameters: csv_file: the file path of wather data \n",
    "    \n",
    "    return: hourly weather data in a dataframe  \n",
    "    \n",
    "    \"\"\"\n",
    "    whole_data = pd.read_csv(csv_file)\n",
    "    whole_data['REPORT_TYPE'] = whole_data['REPORT_TYPE'].astype(str)\n",
    "    whole_data['DATE'] = pd.to_datetime(whole_data['DATE'])\n",
    "    \n",
    "    hourly_data = whole_data[whole_data['REPORT_TYPE'] != 'SOD  ']\n",
    "    hourly_data = hourly_data[['DATE','HourlyPrecipitation','HourlyWindSpeed']]\n",
    "    hourly_data['HourlyPrecipitation'] = pd.to_numeric(hourly_data['HourlyPrecipitation'], errors='coerce')\n",
    "    hourly_data['HourlyWindSpeed'] = pd.to_numeric(hourly_data['HourlyWindSpeed'], errors='coerce')\n",
    "    hourly_data['HourlyPrecipitation'].fillna(0, inplace=True)\n",
    "    hourly_data['HourlyWindSpeed'].fillna(0, inplace=True)\n",
    "    hourly_data = hourly_data.rename(columns={'DATE': 'datetime'})\n",
    "    hourly_data = hourly_data.rename(columns={'HourlyPrecipitation':'hourly_precipitation','HourlyWindSpeed':'hourly_wind_speed'})\n",
    "    \n",
    "    return hourly_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f86c2ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_daily(csv_file):\n",
    "    \"\"\"\n",
    "    This function get and load and clean daily weather data   \n",
    "    \n",
    "    Parameters: csv_file: the file path of weather data \n",
    "    \n",
    "    return: daily data in a dataframe we need \n",
    "    \n",
    "    \"\"\"\n",
    "    whole_data = pd.read_csv(csv_file)\n",
    "    whole_data['REPORT_TYPE'] = whole_data['REPORT_TYPE'].astype(str)\n",
    "    whole_data['DATE'] = pd.to_datetime(whole_data['DATE'])\n",
    "    whole_data['DATE'] = whole_data['DATE'].dt.date\n",
    "    \n",
    "    hourly_data = whole_data[whole_data['REPORT_TYPE'] != 'SOD  ']\n",
    "    hourly_data = hourly_data[['DATE','HourlyPrecipitation','HourlyWindSpeed']]\n",
    "    hourly_data['HourlyPrecipitation'] = pd.to_numeric(hourly_data['HourlyPrecipitation'], errors='coerce')\n",
    "    hourly_data['HourlyWindSpeed'] = pd.to_numeric(hourly_data['HourlyWindSpeed'], errors='coerce')\n",
    "    hourly_data['HourlyPrecipitation'].fillna(0, inplace=True)\n",
    "    hourly_data = hourly_data.groupby('DATE', as_index=False).agg({'HourlyWindSpeed': 'mean', 'HourlyPrecipitation': 'sum'})\n",
    "    \n",
    "    daily_data = whole_data[whole_data['REPORT_TYPE'] == 'SOD  ']\n",
    "    daily_data = daily_data[['DATE','Sunrise','Sunset']]\n",
    "    daily_data['Sunrise'] = pd.to_numeric(daily_data['Sunrise'], errors='coerce')\n",
    "    daily_data['Sunset'] = pd.to_numeric(daily_data['Sunset'], errors='coerce')\n",
    "    \n",
    "    daily_data = hourly_data.merge(daily_data, on='DATE',how='left')\n",
    "    daily_data.fillna(method='ffill',inplace=True)\n",
    "    daily_data.fillna(method='bfill',inplace=True)\n",
    "    daily_data = daily_data.rename(columns={'DATE': 'date'})\n",
    "    daily_data = daily_data.rename(columns={'HourlyPrecipitation':'daily_precipitation', 'HourlyWindSpeed':'daily_wind_speed'})\n",
    "    daily_data = daily_data.rename(columns={'Sunrise': 'sunrise', 'Sunset': 'sunset'})\n",
    "    \n",
    "    return daily_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "202773b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_weather_data():\n",
    "    \"\"\"\n",
    "    This function load and clean daily and hourly weather data   \n",
    "    \n",
    "    Parameters:\n",
    "    \n",
    "    return: daily and hourly weather data in two dataframes\n",
    "    \n",
    "    \"\"\"\n",
    "    hourly_dataframes = []\n",
    "    daily_dataframes = []\n",
    "    \n",
    "    weather_csv_files = glob.glob(os.path.join(os.getcwd(), WEATHER_PATH, \"*.csv\"))\n",
    "    \n",
    "    for csv_file in weather_csv_files:\n",
    "        hourly_dataframe = clean_month_weather_data_hourly(csv_file)\n",
    "        daily_dataframe = clean_month_weather_data_daily(csv_file)\n",
    "        hourly_dataframes.append(hourly_dataframe)\n",
    "        daily_dataframes.append(daily_dataframe)\n",
    "        \n",
    "    # create two dataframes with hourly & daily data from every month\n",
    "    hourly_data = pd.concat(hourly_dataframes)\n",
    "    daily_data = pd.concat(daily_dataframes)\n",
    "    \n",
    "    return hourly_data, daily_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db640ccf",
   "metadata": {},
   "source": [
    "### Process All Data\n",
    "\n",
    "This is executing all the functions and get taxi_data, uber_data and hourly, daily weather data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c60371e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read yellow_tripdata_2015-01\n",
      "read yellow_tripdata_2015-02\n"
     ]
    }
   ],
   "source": [
    "taxi_data = get_and_clean_taxi_data(taxi_zones_df)\n",
    "uber_data = get_uber_data()\n",
    "hourly_weather_data, daily_weather_data = load_and_clean_weather_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f110016",
   "metadata": {},
   "source": [
    "## Part 2: Storing Cleaned Data\n",
    "\n",
    "We then store all the cleaned data in to database, we used SQL to create tables for each dataframe we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40266d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = db.create_engine(DATABASE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9737911a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using SQL to create 4 tables/dataframes\n",
    "HOURLY_WEATHER_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS hourly_weather\n",
    "(\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    datetime DATETIME,\n",
    "    hourly_precipitation FLOAT,\n",
    "    hourly_wind_speed FLOAT\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "DAILY_WEATHER_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS daily_weather\n",
    "(\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    date DATE,\n",
    "    daily_wind_speed FLOAT,\n",
    "    daily_precipitation FLOAT,\n",
    "    sunrise INT32,\n",
    "    sunset INT32\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "TAXI_TRIPS_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS taxi_trips\n",
    "(\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    pickup_datetime DATETIME,\n",
    "    pickup_longitude FLOAT,\n",
    "    pickup_latitude FLOAT,\n",
    "    dropoff_longitude FLOAT,\n",
    "    dropoff_latitude FLOAT,\n",
    "    distance FLOAT\n",
    "    tip_amount FLOAT\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "UBER_TRIPS_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS uber_trips\n",
    "(\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    pickup_datetime DATETIME,\n",
    "    pickup_longitude FLOAT,\n",
    "    pickup_latitude FLOAT,\n",
    "    dropoff_longitude FLOAT,\n",
    "    dropoff_latitude FLOAT,\n",
    "    distance FLOAT\n",
    ");\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692dea1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create that required schema.sql file\n",
    "with open(DATABASE_SCHEMA_FILE, \"w\") as f:\n",
    "    f.write(HOURLY_WEATHER_SCHEMA)\n",
    "    f.write(DAILY_WEATHER_SCHEMA)\n",
    "    f.write(TAXI_TRIPS_SCHEMA)\n",
    "    f.write(UBER_TRIPS_SCHEMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1945a910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the tables with the schema files\n",
    "with engine.connect() as connection:\n",
    "    sql = open(DATABASE_SCHEMA_FILE, \"r\")\n",
    "    commands = sql.read().split(\";\")\n",
    "    sql.close()\n",
    "    for command in commands:\n",
    "        connection.execute(command)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ad1d2c",
   "metadata": {},
   "source": [
    "### Add Data to Database\n",
    "\n",
    "After we create the table we add it with data from dataframe we created above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f08510",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dataframes_to_table(table_to_df_dict):\n",
    "    \"\"\"\n",
    "    This function input data from dataframe to sql table\n",
    "    Parameters:a dictionary contain all dataframes and corresponding table\n",
    "    \n",
    "    \"\"\"\n",
    "    for name, table in table_to_df_dict.items():\n",
    "        table.to_sql(name, engine, if_exists='replace', index=False)\n",
    "        \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69da9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_table_name_to_dataframe = {\n",
    "    \"taxi_trips\": taxi_data,\n",
    "    \"uber_trips\": uber_data,\n",
    "    \"hourly_weather\": hourly_weather_data,\n",
    "    \"daily_weather\": daily_weather_data,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158e103f",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_dataframes_to_table(map_table_name_to_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a632b6",
   "metadata": {},
   "source": [
    "## Part 3: Understanding the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df68d327",
   "metadata": {},
   "source": [
    "This function is getting data by query and write it to a file with filename outfile "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843333ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_query_to_file(query, outfile):\n",
    "    with open(outfile, \"w\") as f:\n",
    "        f.write(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f5a8c5",
   "metadata": {},
   "source": [
    "### Query 1\n",
    "\n",
    "After we stored the data, we can query to answer several question of interests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c22e8a",
   "metadata": {},
   "source": [
    "For 01-2009 through 06-2015, this query is finding hour of the day was the most popular to take a yellow taxi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edab9a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_1 = \"\"\"\n",
    "SELECT strftime('%H', pickup_datetime) AS time, COUNT(*) AS number\n",
    "FROM taxi_trips\n",
    "GROUP BY time\n",
    "ORDER BY number DESC;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a741f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.execute(QUERY_1).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657c34dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_1, \"most_popular_hour.sql\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6b3543",
   "metadata": {},
   "source": [
    "### Query 2\n",
    "For the same time frame, this query finds what day of the week was the most popular to take an uber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca669b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_2 = \"\"\"\n",
    "SELECT strftime('%w', pickup_datetime) AS day, COUNT(*) AS number\n",
    "FROM uber_trips\n",
    "GROUP BY day\n",
    "ORDER BY number DESC;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f095f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.execute(QUERY_2).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035db16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_2, \"most_popular_day.sql\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ce9143",
   "metadata": {},
   "source": [
    "### Query 3\n",
    "This query is finding What is the 95% percentile of distance traveled for all hired trips during July 2013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768464c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_3 = \"\"\"\n",
    "WITH hired_trips AS\n",
    "(\n",
    "    SELECT pickup_datetime, distance FROM taxi_trips \n",
    "    WHERE pickup_datetime BETWEEN '2013-07-01' AND '2013-08-01'\n",
    "    UNION ALL\n",
    "    SELECT pickup_datetime, distance FROM uber_trips\n",
    "    WHERE pickup_datetime BETWEEN '2013-07-01' AND '2013-08-01'\n",
    ")\n",
    "SELECT distance\n",
    "FROM hired_trips\n",
    "ORDER BY distance\n",
    "LIMIT 1\n",
    "OFFSET (SELECT COUNT(*) FROM hired_trips) * 95 / 100 - 1 ;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638e2630",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.execute(QUERY_3).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d477c2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_3, \"95%_percentile_distance.sql\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72354cf",
   "metadata": {},
   "source": [
    "### Query 4\n",
    " This query is finding What were the top 10 days with the highest number of hired rides for 2009, \n",
    " and what was the average distance for each day.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1819cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_4 = \"\"\"\n",
    "WITH hired_trips AS\n",
    "(\n",
    "    SELECT pickup_datetime, distance FROM taxi_trips \n",
    "    WHERE pickup_datetime BETWEEN '2009-01-01' AND '2010-01-01'\n",
    "    UNION ALL\n",
    "    SELECT pickup_datetime, distance FROM uber_trips\n",
    "    WHERE pickup_datetime BETWEEN '2009-01-01' AND '2010-01-01'\n",
    ")\n",
    "SELECT DATE(pickup_datetime) AS date, COUNT(*) AS number, printf(\"%.4f\",AVG(distance)) AS avg_distance\n",
    "FROM hired_trips\n",
    "GROUP BY date\n",
    "ORDER BY number DESC\n",
    "LIMIT 10;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9eed93",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.execute(QUERY_4).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f59690",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_4, \"top_10_days.sql\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b513bd",
   "metadata": {},
   "source": [
    "### Query 5\n",
    "Query 5 is getting Which 10 days in 2014 were the windiest, and how many hired trips were made on those days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06faeae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_5 = \"\"\"\n",
    "WITH hired_trips AS\n",
    "(\n",
    "    SELECT DATE(pickup_datetime) AS date FROM taxi_trips\n",
    "    WHERE date BETWEEN '2014-01-01' AND '2015-01-01'\n",
    "    UNION ALL\n",
    "    SELECT DATE(pickup_datetime) AS date FROM uber_trips\n",
    "    WHERE date BETWEEN '2014-01-01' AND '2015-01-01'\n",
    ")\n",
    "SELECT hired_trips.date as date, printf(\"%.4f\",daily_wind_speed), COUNT(*) AS number\n",
    "FROM hired_trips\n",
    "JOIN daily_weather ON hired_trips.date = DATE(daily_weather.date)\n",
    "GROUP BY hired_trips.date\n",
    "ORDER BY daily_wind_speed DESC\n",
    "LIMIT 10\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4796ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.execute(QUERY_5).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23d3aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_5, \"windiest_10_days.sql\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afad864",
   "metadata": {},
   "source": [
    "### Query 6\n",
    "This query is finding that during Hurricane Sandy in NYC (Oct 29-30, 2012) and the week leading up to it, how many trips were taken each hour, and for each hour, how much precipitation did NYC receive and what was the sustained wind speed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed2963e",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_6 = \"\"\"\n",
    "WITH hired_trips AS\n",
    "(\n",
    "    SELECT strftime('%Y-%m-%d %H',pickup_datetime) AS trip_hour FROM taxi_trips\n",
    "    WHERE pickup_datetime BETWEEN '2012-10-22' AND '2012-10-31'\n",
    "    UNION ALL\n",
    "    SELECT strftime('%Y-%m-%d %H',pickup_datetime) AS trip_hour FROM uber_trips\n",
    "    WHERE pickup_datetime BETWEEN '2012-10-22' AND '2012-10-31'\n",
    "),\n",
    "hurricane_weather AS\n",
    "(\n",
    "    SELECT strftime('%Y-%m-%d %H',datetime) AS weather_hour, hourly_precipitation, hourly_wind_speed FROM hourly_weather\n",
    "    WHERE datetime BETWEEN '2012-10-22' AND '2012-10-31'\n",
    ")\n",
    "SELECT weather_hour, COALESCE(COUNT(trip_hour),0) AS number, hourly_precipitation, hourly_wind_speed\n",
    "FROM hurricane_weather\n",
    "LEFT JOIN hired_trips\n",
    "ON trip_hour = weather_hour\n",
    "GROUP BY weather_hour\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe768ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.execute(QUERY_6).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76939446",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_6, \"hurricane_sandy.sql\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8ca9cc",
   "metadata": {},
   "source": [
    "## Part 4: Visualizing the Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4495139",
   "metadata": {},
   "source": [
    "### Visualization 1\n",
    "\n",
    "We are going to visualize what we found in previous questions.\n",
    "\n",
    "\n",
    "We makes use of the `matplotlib` library. There are other libraries, including `pandas` built-in plotting library, kepler for geospatial data representation, `seaborn`, and others._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbe26e2",
   "metadata": {},
   "source": [
    "This function is getting the graph for yellow taxi trips in every hour "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9a2cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def taxi_trip_by_hour():\n",
    "    \"\"\"\n",
    "    This function plot number of trips per hour\n",
    "    \"\"\"\n",
    "    # use the function read_sql_query to pull out values needed to plot and make it a dataframe\n",
    "    values = pd.read_sql_query(QUERY_1, engine).sort_values(by=\"time\", ascending=True)\n",
    "    values.plot.bar(x = 'time',title=\"Yellow Taxi Trips per Hour\", xlabel=\"Hour\", ylabel=\"Number of Trips\")   \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f99f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_trip_by_hour()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458bbd43",
   "metadata": {},
   "source": [
    "### Visualization 2\n",
    "This function shows the average distance traveled per month\n",
    "In order to find this for both taxis and Ubers combined, we need to create two querys to get taxi and uber data seperatly from two tables. And include the 90% confidence interval around the mean in the visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb76a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_taxi = \"\"\"SELECT strftime('%Y-%m', pickup_datetime) AS month, SUM(distance) AS distance\n",
    "                FROM taxi_trips\n",
    "                GROUP BY month\n",
    "             \"\"\"\n",
    "QUERY_uber =  \"\"\"SELECT strftime('%Y-%m', pickup_datetime) AS month, SUM(distance) AS distance\n",
    "                FROM uber_trips\n",
    "               GROUP BY month\n",
    "             \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49a7592",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_distance_month():\n",
    "    \"\"\"\n",
    "    This function plot average distance traveled per month and a 90% confidence interval for mean\n",
    "    \"\"\"\n",
    "    # use the function read_sql_query to pull out values needed to plot and make it a dataframe\n",
    "    df_taxi = pd.read_sql_query(QUERY_taxi, engine)\n",
    "    df_uber = pd.read_sql_query(QUERY_uber, engine)\n",
    "    \n",
    "    df_taxi['month'] = pd.to_datetime(df_taxi['month'], format='%Y-%m')\n",
    "    df_uber['month'] = pd.to_datetime(df_uber['month'], format='%Y-%m')\n",
    "\n",
    "    taxi_info =df_taxi['distance'].groupby(df_taxi['month'].dt.month).agg(['mean','count','sem'])\n",
    "    uber_info =df_uber['distance'].groupby(df_uber['month'].dt.month).agg(['mean','count','sem'])\n",
    "    taxi_low, taxi_high = st.t.interval(0.90,taxi_info['count']-1 , loc=taxi_info['mean'], scale=taxi_info['sem'])\n",
    "    uber_low, uber_high = st.t.interval(0.90, uber_info['count']-1, loc=uber_info['mean'], scale=uber_info['sem'])\n",
    "    taxi_info['mean'].plot( color='orange', label='Taxi',ylabel = 'Average Distance')\n",
    "    uber_info['mean'].plot( color='purple', label='Uber',ylabel = 'Average Distance')\n",
    "    plt.fill_between(taxi_info.index, taxi_low, taxi_high, color='orange', alpha=0.2)\n",
    "    plt.fill_between(uber_info.index, uber_low, uber_high, color='purple', alpha=0.2)\n",
    "    plt.title('Average Distance per Month')\n",
    "    plt.legend()\n",
    "\n",
    "   \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929ccde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_distance_month()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e383e955",
   "metadata": {},
   "source": [
    "### Visualization 3\n",
    "\n",
    "We found three lat/long coordinate boxes around the three major New York airports: LGA, JFK, and EWR.\n",
    "Created a visualization that compares what day of the week was most popular for drop offs for each airport.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d2b6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_pop_airport():\n",
    "    \"\"\"\n",
    "    This function plot number of dropoff for each airport \n",
    "    \"\"\"\n",
    "    \n",
    "    df = []\n",
    "    EWR = [-74.192390,40.670047,-74.153337,40.707844,'EWR']\n",
    "    JFK=  [ -73.823776,40.620946,-73.747559,40.665083,'JFK']\n",
    "    LGA = [  -73.889329,40.766710,-73.854761,40.786449,'LGA']\n",
    "    for place in [EWR,JFK,LGA]:\n",
    "        query_place = f'''WITH trips AS (SELECT pickup_datetime, dropoff_longitude, dropoff_latitude FROM taxi_trips\n",
    "                    UNION ALL SELECT pickup_datetime,dropoff_longitude,dropoff_latitude FROM uber_trips)\n",
    "\n",
    "                        SELECT strftime('%w', pickup_datetime) AS day ,COUNT(*) AS {place[4]}\n",
    "                        FROM trips\n",
    "                        WHERE dropoff_longitude BETWEEN {place[0]} AND {place[2]} AND dropoff_latitude BETWEEN {place[1]} AND {place[3]}\n",
    "                        GROUP BY day\n",
    "              '''\n",
    "        df.append(pd.read_sql_query(query_place, engine))\n",
    "        \n",
    "    JFK = df[1]['JFK']\n",
    "    EWR =df[0]['EWR']\n",
    "    LGA =df[2]['LGA']\n",
    "    df[0]['JFK'] = df[1]['JFK']\n",
    "    df[0]['LGA'] = df[2]['LGA']\n",
    "\n",
    "    df[0].plot(x=\"day\", y=['EWR','JFK','LGA'], kind=\"bar\", title=\"Dropoffs for Each Airport in Each Day of a Week\", xlabel=\"Day\", ylabel=\"Number of Dropoffs\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216068d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_pop_airport()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6a7f59",
   "metadata": {},
   "source": [
    "### Visualization 4\n",
    "Create a heatmap of all hired trips over a map of the area. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164c6c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmap_trips():\n",
    "    \"\"\"\n",
    "    This function create a heatmap of number of trips in the area \n",
    "    \"\"\"\n",
    "    \n",
    "    query = '''SELECT pickup_longitude, pickup_latitude\n",
    "               FROM taxi_trips\n",
    "               UNION ALL\n",
    "               SELECT pickup_longitude, pickup_latitude\n",
    "               FROM uber_trips\n",
    "            '''\n",
    "    df = pd.read_sql_query(query, engine)\n",
    "    zones = taxi_zones_df.copy()\n",
    "    zones = zones[['LocationID','geometry']]\n",
    "    df = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.pickup_longitude, df.pickup_latitude))\n",
    "    result = zones.sjoin(df, how='left')\n",
    "    result = result.set_index('LocationID')\n",
    "    result['num'] = result.groupby('LocationID')['geometry'].count()\n",
    "    result['log_num'] = np.log(result['num'])\n",
    "    result.plot(column='log_num', cmap='plasma', legend=True, legend_kwds={'label': \"number of trips in different area\"})\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e176c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_trips()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f15d044",
   "metadata": {},
   "source": [
    "### Visualization 5\n",
    "CreateD a scatter plot that compares tip amount versus distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff0d89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tip_distance():\n",
    "    \"\"\"\n",
    "    This function create a scatter plot for tip amount and distance \n",
    "    \"\"\"\n",
    "\n",
    "    QUERY = \"\"\" SELECT tip_amount,distance\n",
    "                FROM taxi_trips      \n",
    "            \"\"\"\n",
    "    df = pd.read_sql_query(QUERY, engine)\n",
    "\n",
    "    # sample the data to make the scatter plot clearer\n",
    "    df =df.sample(500)\n",
    "    df = df[df['distance'] < 100]\n",
    "    df =df[df['tip_amount'] > 0]\n",
    "    df.plot(x=\"distance\", y=\"tip_amount\", kind=\"scatter\", title=\"Tip Amount with Distance\", xlabel=\"Distance\", ylabel=\"Tip Amount\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d173d942",
   "metadata": {},
   "outputs": [],
   "source": [
    "tip_distance()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa7395f",
   "metadata": {},
   "source": [
    "### Visualization 5\n",
    "Created another scatter plot that compares tip amount versus precipitation amount for Yellow Taxi rides. You may remove any outliers how you see fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a75592",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tip_precipitation():\n",
    "    \"\"\"\n",
    "    This function create a scatter plot for tip amount and precipitation \n",
    "    \"\"\"\n",
    "    query_tip =\"\"\"SELECT strftime('%Y-%m-%d %H', pickup_datetime) AS DATE, tip_amount\n",
    "                FROM taxi_trips\n",
    "                \"\"\" \n",
    "    query_precipitation =\"\"\"SELECT strftime('%Y-%m-%d %H', datetime) as DATE, hourly_precipitation\n",
    "                FROM hourly_weather\n",
    "                \"\"\"\n",
    "    df_tip = pd.read_sql_query(query_tip, engine)\n",
    "    df_precipitation =pd.read_sql_query(query_precipitation, engine)\n",
    "    df = df_tip.merge(df_precipitation,on = 'DATE').sample(2000)\n",
    "    df = df[df['hourly_precipitation'] >0]\n",
    "    df =df[df['tip_amount'] > 0]\n",
    "    df.plot(x=\"tip_amount\", y=\"hourly_precipitation\", kind=\"scatter\", title=\"Tip Amount and Precipitation\", \n",
    "            xlabel=\"Tip Amount\", ylabel=\"Hourly Precipitation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38f5ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tip_precipitation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47906ddf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
